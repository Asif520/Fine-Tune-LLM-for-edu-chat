{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "659ef657-b294-499e-ad97-4ffca46c14a3",
    "_uuid": "19ccf80e-3710-4c77-aaac-bbecf2de3979",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:13:38.797196Z",
     "iopub.status.busy": "2025-10-28T00:13:38.796906Z",
     "iopub.status.idle": "2025-10-28T00:14:04.857947Z",
     "shell.execute_reply": "2025-10-28T00:14:04.857378Z",
     "shell.execute_reply.started": "2025-10-28T00:13:38.797170Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 00:13:51.100439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761610431.335468      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761610431.398806      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os,json, random, torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,\n",
    "    Trainer, TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import accelerate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "7e8d7118-5369-458e-af15-9f5cf8d84aac",
    "_uuid": "863a64a7-770b-4c54-89e6-8bf257e7252e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:04.859947Z",
     "iopub.status.busy": "2025-10-28T00:14:04.859459Z",
     "iopub.status.idle": "2025-10-28T00:14:04.863653Z",
     "shell.execute_reply": "2025-10-28T00:14:04.862941Z",
     "shell.execute_reply.started": "2025-10-28T00:14:04.859929Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1c6199a1-4e6c-4ce3-874b-90a6b9e00075",
    "_uuid": "095dcffb-f698-472a-9e4f-dbba06cc745d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:04.864745Z",
     "iopub.status.busy": "2025-10-28T00:14:04.864441Z",
     "iopub.status.idle": "2025-10-28T00:14:04.883766Z",
     "shell.execute_reply": "2025-10-28T00:14:04.883067Z",
     "shell.execute_reply.started": "2025-10-28T00:14:04.864728Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "67d5315f-9392-42e7-8ab7-82dd2f033617",
    "_uuid": "efb8adb6-cce3-4cbd-a87c-2e758958a0b6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:04.884718Z",
     "iopub.status.busy": "2025-10-28T00:14:04.884476Z",
     "iopub.status.idle": "2025-10-28T00:14:04.897947Z",
     "shell.execute_reply": "2025-10-28T00:14:04.897357Z",
     "shell.execute_reply.started": "2025-10-28T00:14:04.884696Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# USER CONFIG\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "DATA_PATH = \"/kaggle/input/study-dataset-v2/study_dataset_v2_large_30000.json\"      # expects keys: instruction, response\n",
    "OUTPUT_DIR = \"kaggle/working//study-finetuned-lora-selfcontext\"\n",
    "\n",
    "SEED = 42\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "BATCH_SIZE = 8               # per-device batch\n",
    "GRAD_ACCUM = 8               # gradient accumulation -> effective batch = BATCH_SIZE * GRAD_ACCUM * n_gpus\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "WEIGHT_DECAY = 0.0\n",
    "WARMUP_STEPS = 100\n",
    "MAX_LENGTH = 512             # input (prompt+response) truncation length\n",
    "VAL_SPLIT = 0.05             # fraction for validation\n",
    "MAX_RESP_TOKENS = 200        # generation length for eval (how long replies should be)\n",
    "USE_4BIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "843d2833-9ac2-44c9-97fc-fd2d9c72d173",
    "_uuid": "645571c8-f223-4608-98f4-9a16f7449057",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:04.899956Z",
     "iopub.status.busy": "2025-10-28T00:14:04.899749Z",
     "iopub.status.idle": "2025-10-28T00:14:05.172812Z",
     "shell.execute_reply": "2025-10-28T00:14:05.172166Z",
     "shell.execute_reply.started": "2025-10-28T00:14:04.899943Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "from huggingface_hub import login\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "67eb7bec-d017-4883-889c-e7b382e7b5ec",
    "_uuid": "9c0d8efe-8807-4182-9695-ca0d440720ba",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.173838Z",
     "iopub.status.busy": "2025-10-28T00:14:05.173541Z",
     "iopub.status.idle": "2025-10-28T00:14:05.177419Z",
     "shell.execute_reply": "2025-10-28T00:14:05.176817Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.173821Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# System prompt that will always be included\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a knowledgeable, friendly AI tutor for high-school and college students. \"\n",
    "    \"You understand science, commerce, and humanities topics. \"\n",
    "    \"When a student asks a question, explain clearly using reasoning and examples, \"\n",
    "    \"without requiring extra context. Be conversational and helpful.\"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"### System:\\n{system}\\n\\n\"\n",
    "    \"### Question:\\n{instruction}\\n\\n\"\n",
    "    \"### Answer:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "791402a1-4080-487c-bcde-80d05ace8c5c",
    "_uuid": "fd151e8d-7e53-4411-827b-40eca4e8ffb9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.178381Z",
     "iopub.status.busy": "2025-10-28T00:14:05.178154Z",
     "iopub.status.idle": "2025-10-28T00:14:05.198313Z",
     "shell.execute_reply": "2025-10-28T00:14:05.197718Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.178365Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "8e770a77-c0c5-4b7b-a6c9-69b3be292d2b",
    "_uuid": "e1ec1fd6-df91-4cfc-be83-f4a0c9c2e2a3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.199416Z",
     "iopub.status.busy": "2025-10-28T00:14:05.199126Z",
     "iopub.status.idle": "2025-10-28T00:14:05.204313Z",
     "shell.execute_reply": "2025-10-28T00:14:05.203460Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.199401Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_prompt(instruction: str) -> str:\n",
    "    return PROMPT_TEMPLATE.format(system=SYSTEM_PROMPT, instruction=instruction.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "5dff72ca-21f2-42b5-935d-86f77ba60e97",
    "_uuid": "fec0580e-9c48-4ee5-9810-edb9d0076f28",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.205287Z",
     "iopub.status.busy": "2025-10-28T00:14:05.205077Z",
     "iopub.status.idle": "2025-10-28T00:14:05.218297Z",
     "shell.execute_reply": "2025-10-28T00:14:05.217627Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.205272Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "def load_local_json_as_dataset(path: str) -> Dataset:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    records = []\n",
    "    for obj in data:\n",
    "        if not obj.get(\"instruction\") or not obj.get(\"response\"):\n",
    "            continue\n",
    "        records.append({\n",
    "            \"instruction\": obj[\"instruction\"],\n",
    "            \"response\": obj[\"response\"]\n",
    "        })\n",
    "    return Dataset.from_list(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "ad4d2546-8723-4ea6-a325-fb629f043fc2",
    "_uuid": "aa068dbc-c86a-4d6b-aebb-7ac470269d95",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.219292Z",
     "iopub.status.busy": "2025-10-28T00:14:05.219029Z",
     "iopub.status.idle": "2025-10-28T00:14:05.722375Z",
     "shell.execute_reply": "2025-10-28T00:14:05.721768Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.219272Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_ds = load_local_json_as_dataset(DATA_PATH)\n",
    "raw_ds = raw_ds.train_test_split(test_size=VAL_SPLIT, seed=SEED)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": raw_ds[\"train\"].shuffle(seed=SEED),\n",
    "    \"validation\": raw_ds[\"test\"].shuffle(seed=SEED)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "3d39da36-cbc5-448e-b63d-71549b2f5d72",
    "_uuid": "614f9132-3c0b-457d-8694-8b16fdea39bf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.723140Z",
     "iopub.status.busy": "2025-10-28T00:14:05.722971Z",
     "iopub.status.idle": "2025-10-28T00:14:05.728781Z",
     "shell.execute_reply": "2025-10-28T00:14:05.728065Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.723125Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'response'],\n",
       "        num_rows: 28500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'response'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.729840Z",
     "iopub.status.busy": "2025-10-28T00:14:05.729598Z",
     "iopub.status.idle": "2025-10-28T00:14:05.947509Z",
     "shell.execute_reply": "2025-10-28T00:14:05.946821Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.729820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 28500\n",
      "Validation examples: 1500\n",
      "CUDA available: True\n",
      "GPU count: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset size and GPU availability\n",
    "print(f\"Training examples: {len(dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "fe6aa076-ba17-4fbb-9b04-9faad6d7d7e5",
    "_uuid": "1c4d6138-69f7-4378-a3fa-76cd2fce9aec",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.948426Z",
     "iopub.status.busy": "2025-10-28T00:14:05.948189Z",
     "iopub.status.idle": "2025-10-28T00:14:05.961546Z",
     "shell.execute_reply": "2025-10-28T00:14:05.960942Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.948410Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "f04d178c-d17e-4c59-86a7-44650374a778",
    "_uuid": "ce9428d5-25ba-4840-b6da-881785dbe107",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:05.964425Z",
     "iopub.status.busy": "2025-10-28T00:14:05.964049Z",
     "iopub.status.idle": "2025-10-28T00:14:07.671969Z",
     "shell.execute_reply": "2025-10-28T00:14:07.671079Z",
     "shell.execute_reply.started": "2025-10-28T00:14:05.964407Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9230dff02d048328d40e6bd3b886a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26190682b4a048b2beb708f5de082b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e8268be8f04cc3b5d27fa44e326d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer & Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "if tokenizer.eos_token is None:\n",
    "    tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "8f75c998-d60d-4f60-82bc-899c95ff8ae4",
    "_uuid": "de060e18-362c-4e64-b783-661f285c7f88",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:14:07.673717Z",
     "iopub.status.busy": "2025-10-28T00:14:07.672900Z",
     "iopub.status.idle": "2025-10-28T00:17:12.644986Z",
     "shell.execute_reply": "2025-10-28T00:17:12.644428Z",
     "shell.execute_reply.started": "2025-10-28T00:14:07.673696Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7982a2d1645e4da48a1a46b1d6cc54f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d839e79c49d4ca3b9c0e147495de2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c8a4e21077448d9bc43da58eab77f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0a5e9d8a72414983ddbca6ed598b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3587b5f7f814672b02e85306e9c4531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32968874db40443eaf5d4f3dd18ff229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1905e62cff498f8412ce316feb4eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f8670d42424b969d62032f3cf23f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59143469d7ae4a78b458cc727c13c262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "0898d6a5-cedb-4544-b526-ebf8080e2c38",
    "_uuid": "d80d3ba3-95ff-4bb7-97c6-9bf596075240",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:12.645942Z",
     "iopub.status.busy": "2025-10-28T00:17:12.645732Z",
     "iopub.status.idle": "2025-10-28T00:17:15.460471Z",
     "shell.execute_reply": "2025-10-28T00:17:15.459827Z",
     "shell.execute_reply.started": "2025-10-28T00:17:12.645926Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "48e0d9e7-bcf6-42e2-b219-7f14161d9517",
    "_uuid": "b50d0f23-bce9-4e77-9455-ced905cf9359",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:15.461362Z",
     "iopub.status.busy": "2025-10-28T00:17:15.461134Z",
     "iopub.status.idle": "2025-10-28T00:17:15.465301Z",
     "shell.execute_reply": "2025-10-28T00:17:15.464442Z",
     "shell.execute_reply.started": "2025-10-28T00:17:15.461346Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "6870f7a9-1069-4089-917a-91b5d3e0d8e3",
    "_uuid": "752be104-7f06-4749-9354-dd37bcbf9556",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:15.466194Z",
     "iopub.status.busy": "2025-10-28T00:17:15.465967Z",
     "iopub.status.idle": "2025-10-28T00:17:15.649725Z",
     "shell.execute_reply": "2025-10-28T00:17:15.649017Z",
     "shell.execute_reply.started": "2025-10-28T00:17:15.466177Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters added successfully.\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "print(\"LoRA adapters added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "3c47cfed-aa2a-40cc-b9aa-5bd14c8b9d3f",
    "_uuid": "d1d2173a-2be1-4fc2-a3df-21f765a54acc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:15.650761Z",
     "iopub.status.busy": "2025-10-28T00:17:15.650497Z",
     "iopub.status.idle": "2025-10-28T00:17:15.655785Z",
     "shell.execute_reply": "2025-10-28T00:17:15.655148Z",
     "shell.execute_reply.started": "2025-10-28T00:17:15.650739Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "def tokenize_example(ex,tokenizer):\n",
    "    prompt = build_prompt(ex[\"instruction\"])\n",
    "    full = prompt + ex[\"response\"].strip() + tokenizer.eos_token\n",
    "    full_tokens = tokenizer(full, truncation=True, max_length=MAX_LENGTH)\n",
    "    prompt_tokens = tokenizer(prompt, truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    input_ids = full_tokens[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # mask system + question tokens from loss\n",
    "    prompt_len = len(prompt_tokens[\"input_ids\"])\n",
    "    for i in range(prompt_len):\n",
    "        if i < len(labels):\n",
    "            labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": full_tokens[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "e21611c5-41fe-494d-bff1-de3f1a4f3a2a",
    "_uuid": "39e9dd49-80ab-4855-9262-5227e9cd621d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:15.656642Z",
     "iopub.status.busy": "2025-10-28T00:17:15.656420Z",
     "iopub.status.idle": "2025-10-28T00:17:15.672746Z",
     "shell.execute_reply": "2025-10-28T00:17:15.671917Z",
     "shell.execute_reply.started": "2025-10-28T00:17:15.656626Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch):\n",
    "    # batch is a dict of lists\n",
    "    out = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    for i in range(len(batch[\"instruction\"])):\n",
    "        ex = {\n",
    "            \"instruction\": batch[\"instruction\"][i],\n",
    "            \"response\": batch[\"response\"][i]\n",
    "        }\n",
    "        tok = tokenize_example(ex, tokenizer)\n",
    "        out[\"input_ids\"].append(tok[\"input_ids\"])\n",
    "        out[\"attention_mask\"].append(tok[\"attention_mask\"])\n",
    "        out[\"labels\"].append(tok[\"labels\"])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "d1e54e34-7862-4389-bc56-839079a92f4c",
    "_uuid": "0aae3bfb-dbf6-48e9-b7e4-d1fc0fa57291",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:15.673741Z",
     "iopub.status.busy": "2025-10-28T00:17:15.673527Z",
     "iopub.status.idle": "2025-10-28T00:17:36.527312Z",
     "shell.execute_reply": "2025-10-28T00:17:36.526681Z",
     "shell.execute_reply.started": "2025-10-28T00:17:15.673725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44989ec6b9384c70854876c948ea646e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14831d83e11e42a4adea0ab21232bbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = dataset.map(lambda batch: preprocess_batch(batch), batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "a36e721c-ddb5-4621-8e40-ae17fd70a9a1",
    "_uuid": "c62956e4-504a-4d98-b54a-3b6babfac825",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:36.528378Z",
     "iopub.status.busy": "2025-10-28T00:17:36.528078Z",
     "iopub.status.idle": "2025-10-28T00:17:36.534028Z",
     "shell.execute_reply": "2025-10-28T00:17:36.533335Z",
     "shell.execute_reply.started": "2025-10-28T00:17:36.528353Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForCausal:\n",
    "    tokenizer: AutoTokenizer\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
    "        attention_mask = [torch.tensor(f[\"attention_mask\"]) for f in features]\n",
    "        labels = [torch.tensor(f[\"labels\"]) for f in features]\n",
    "        batch = {\n",
    "            \"input_ids\": torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id),\n",
    "            \"attention_mask\": torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0),\n",
    "            \"labels\": torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100),\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "b93d7465-cbd3-47e8-a732-bb37a8540b05",
    "_uuid": "831de74c-c4cb-45d9-8138-d7d67c486580",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:36.534993Z",
     "iopub.status.busy": "2025-10-28T00:17:36.534760Z",
     "iopub.status.idle": "2025-10-28T00:17:36.549965Z",
     "shell.execute_reply": "2025-10-28T00:17:36.549259Z",
     "shell.execute_reply.started": "2025-10-28T00:17:36.534971Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCausal(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "d6acf849-6800-493c-aaed-ccadd4a6a2df",
    "_uuid": "66d47a6e-69e1-4ded-a4f0-cac570fb7f4d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:36.550952Z",
     "iopub.status.busy": "2025-10-28T00:17:36.550683Z",
     "iopub.status.idle": "2025-10-28T00:17:36.564080Z",
     "shell.execute_reply": "2025-10-28T00:17:36.563421Z",
     "shell.execute_reply.started": "2025-10-28T00:17:36.550937Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "total_train_batch = BATCH_SIZE * GRAD_ACCUM\n",
    "logging_steps = 80\n",
    "save_steps = 500\n",
    "eval_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "e652e5f3-68c8-49d7-a8c4-c7aff17f6993",
    "_uuid": "da7b0d5b-e02b-4bd2-9f2a-6a3da087b541",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:36.564888Z",
     "iopub.status.busy": "2025-10-28T00:17:36.564704Z",
     "iopub.status.idle": "2025-10-28T00:17:36.608136Z",
     "shell.execute_reply": "2025-10-28T00:17:36.607422Z",
     "shell.execute_reply.started": "2025-10-28T00:17:36.564874Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TrainingArguments & Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    dataloader_num_workers=3,  # Enable multi-worker data loading\n",
    "    dataloader_pin_memory=True,\n",
    "    max_grad_norm=0.3,  # Gradient clipping for stability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "8e25399c-a1b0-449d-8869-6e8f19d0b2dd",
    "_uuid": "fce9e443-3287-445c-bddb-bccb8ba485e3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:36.609106Z",
     "iopub.status.busy": "2025-10-28T00:17:36.608886Z",
     "iopub.status.idle": "2025-10-28T00:17:36.633765Z",
     "shell.execute_reply": "2025-10-28T00:17:36.633243Z",
     "shell.execute_reply.started": "2025-10-28T00:17:36.609091Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "099bc29d-a69f-442d-b768-3a8f16f7067e",
    "_uuid": "a24dfdd5-2d9d-4e38-8050-689083c0f66b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T00:17:36.634622Z",
     "iopub.status.busy": "2025-10-28T00:17:36.634393Z",
     "iopub.status.idle": "2025-10-28T11:09:27.235416Z",
     "shell.execute_reply": "2025-10-28T11:09:27.233881Z",
     "shell.execute_reply.started": "2025-10-28T00:17:36.634602Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='892' max='1338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 892/1338 10:51:08 < 5:26:18, 0.02 it/s, Epoch 2.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2735\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2737\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "20a4cd5e-89f1-4c5e-aa5c-5807426fd490",
    "_uuid": "5b270187-43e4-45f5-92d1-41837e1df2df",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T11:10:34.900004Z",
     "iopub.status.busy": "2025-10-28T11:10:34.899471Z",
     "iopub.status.idle": "2025-10-28T11:10:34.903418Z",
     "shell.execute_reply": "2025-10-28T11:10:34.902779Z",
     "shell.execute_reply.started": "2025-10-28T11:10:34.899982Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "31a06c8c-8f52-48c1-883d-6c619f390a45",
    "_uuid": "e116f3a1-dfa7-488b-b601-2c313576a15b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T11:10:37.991023Z",
     "iopub.status.busy": "2025-10-28T11:10:37.990391Z",
     "iopub.status.idle": "2025-10-28T11:10:47.780818Z",
     "shell.execute_reply": "2025-10-28T11:10:47.779903Z",
     "shell.execute_reply.started": "2025-10-28T11:10:37.990996Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PEFT/LoRA weights to: kaggle/working//study-finetuned-lora-selfcontext/adapter_model.bin\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    peft_state = get_peft_model_state_dict(model)\n",
    "    peft_path = os.path.join(OUTPUT_DIR, \"adapter_model.bin\")\n",
    "    torch.save(peft_state, peft_path)\n",
    "    print(\"Saved PEFT/LoRA weights to:\", peft_path)\n",
    "except Exception as e:\n",
    "    print(\"Warning: couldn't save PEFT state dict separately:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "c2ded8ee-d698-4d6b-a8b5-3968a18dc4a6",
    "_uuid": "0b5d5825-b716-4c2a-9c0d-9cb8dfbf32c9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-28T11:27:20.547798Z",
     "iopub.status.busy": "2025-10-28T11:27:20.547147Z",
     "iopub.status.idle": "2025-10-28T11:27:33.280154Z",
     "shell.execute_reply": "2025-10-28T11:27:33.279103Z",
     "shell.execute_reply.started": "2025-10-28T11:27:20.547771Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kaggle/working//study-finetuned-lora-selfcontext/tokenizer_config.json',\n",
       " 'kaggle/working//study-finetuned-lora-selfcontext/special_tokens_map.json',\n",
       " 'kaggle/working//study-finetuned-lora-selfcontext/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save LoRA weights\n",
    "model.save_pretrained(OUTPUT_DIR)  # This saves LoRA + config\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T11:32:51.967758Z",
     "iopub.status.busy": "2025-10-28T11:32:51.966948Z",
     "iopub.status.idle": "2025-10-28T11:32:51.972126Z",
     "shell.execute_reply": "2025-10-28T11:32:51.971560Z",
     "shell.execute_reply.started": "2025-10-28T11:32:51.967732Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "peft_config.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:00:26.197026Z",
     "iopub.status.busy": "2025-10-28T12:00:26.196774Z",
     "iopub.status.idle": "2025-10-28T12:00:26.200843Z",
     "shell.execute_reply": "2025-10-28T12:00:26.199986Z",
     "shell.execute_reply.started": "2025-10-28T12:00:26.197009Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/kaggle/working/finetuned-lora-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:03:13.837655Z",
     "iopub.status.busy": "2025-10-28T12:03:13.837021Z",
     "iopub.status.idle": "2025-10-28T12:03:13.841710Z",
     "shell.execute_reply": "2025-10-28T12:03:13.840897Z",
     "shell.execute_reply.started": "2025-10-28T12:03:13.837629Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:04:32.521828Z",
     "iopub.status.busy": "2025-10-28T12:04:32.521086Z",
     "iopub.status.idle": "2025-10-28T12:04:42.377870Z",
     "shell.execute_reply": "2025-10-28T12:04:42.376866Z",
     "shell.execute_reply.started": "2025-10-28T12:04:32.521793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True LoRA weights saved: /kaggle/working/finetuned-lora-model/adapter_model.bin → 4230.0 MB\n"
     ]
    }
   ],
   "source": [
    "lora_state_dict = get_peft_model_state_dict(model)\n",
    "lora_path = os.path.join(OUTPUT_DIR, \"adapter_model.bin\")\n",
    "torch.save(lora_state_dict, lora_path)\n",
    "\n",
    "# 2. Save adapter config (critical!)\n",
    "from peft import LoraConfig\n",
    "peft_config.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# 3. Save tokenizer\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"True LoRA weights saved: {lora_path} → {os.path.getsize(lora_path) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8544312,
     "sourceId": 13460865,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
